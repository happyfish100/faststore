#
# the server group is a physical concept, the data under the same server group
# is the same (redundant or backup).
#
# the data group is a logical or virtual concept, its purpose is to facilitate
# cluster expansion.
#
# cluster expansion: migrate one or more data group(s) to one or more new server group(s).
# data migration: restart after the mappings from data group to server group modified,
# the data replication will be completed automatically.
#
# in order to facilitate cluster expansion, there is a one to many relationship
# between the server groups and the data groups.
# 1 : 64 (server groups : data groups) is recommended.
#
# data is hashed to the server group according to block hash code % the data groups.
# once the number of data groups is set, it can NOT be changed, otherwise
# the data access will be confused!
#
# if you want to change the number of data groups after the formal running of
# the cluster, you MUST create a new cluster and migrate the data to it.

# the group count of the servers / instances
server_group_count = 1

# all data groups must be mapped to the server group(s) without omission.
# once the number of data groups is set, it can NOT be changed, otherwise
# the data access will be confused!
data_group_count = 64

# config the auth config filename
auth_config_filename = ../auth/auth.conf

[leader-election]
# the quorum for leader election
# set quorum to majority to avoid brain-split
# value list:
##  any: no requirement
##  majority: more than half
##  auto: set to majority when the number of nodes is odd,
##        otherwise set to any
# default value is auto
quorum = auto

# the timeout to determinate leader lost
# the default value is 3 seconds
leader_lost_timeout = 3

# the max wait time for leader election
# this parameter is for the leader restart
# the default value is 30 seconds
max_wait_time = 30

[master-election]
# if resume the master role when the preseted server become available
# set this parameter to true for load balance
# default value is true
resume_master_role = true

# weather one slave upgrade to master when the master offline
# set this parameter to false to avoid data inconsistency
# when the network partition occurs
# default value is true
failover = true

# the policy to elect master when failover is true
# normally the server with highest data version is elected as the master
# the value list:
## strict: the master MUST be the server with highest data version
## timeout: wait the the server with highest data version until timeout
####        the timeout format is timeout[:seconds], such as timeout:30,
####        the default timeout is 60 seconds when timeout keyword only
# default value is strict
policy = strict

[group-cluster]
# the default cluster port
port = 21014

[group-replica]
# the default replica port
port = 21015

[group-service]
# the default service port
port = 21016

## Important:server group mark, don't modify this line.

# the server group id based 1
# the data under the same server group is the same (redundant or backup)
[server-group-1]

# config one or more server id(s)
## multiple server ids separated by comma(s).
## [start, end] for range, including start and end.
# this parameter can occurs more than once.
# server_ids = [1, 3]
server_ids = 1

# the data group id based 1. the formats as:
##  * multiple data group ids separated by comma(s).
##  * [start, end] for range, including start and end.
# this parameter can occurs more than once.
data_group_ids = [1, 32]
data_group_ids = [33, 64]

# config a server
# section format: [server-$id]
# server id is a 32 bits natural number (1, 2, 3 etc.),
[server-1]

# format: host[:port]
# host can be an IP address or a hostname
# IP address is recommended
# can occur more than once
host = 172.16.168.128

# [server-2]
# host = 172.16.168.129

# [server-3]
# host = 172.16.168.130
